{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f718c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as alb\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing images\n",
    "IMAGES_PATH = os.path.join('data', 'images')\n",
    "\n",
    "# Number of images to process\n",
    "number_images = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0af713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct camera index\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop for capturing images\n",
    "for imgnum in range(number_images):\n",
    "    print('Collecting image {}'.format(imgnum))\n",
    "\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    print(f'Ret: {ret}')\n",
    "\n",
    "    # Check if the frame was captured successfully\n",
    "    if ret:\n",
    "        # Generate a unique image name\n",
    "        imgname = os.path.join(IMAGES_PATH, f'{str(uuid.uuid1())}.jpg')\n",
    "        \n",
    "        # Save the captured frame as an image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        \n",
    "        # Display the captured frame\n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "        # Pause for a short duration\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Check if the 'q' key was pressed to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        print('Camera capture failed!')\n",
    "        break\n",
    "\n",
    "# Release the camera and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c290310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate with labelme\n",
    "!labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU memory consumption growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available GPUs\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print('Available GPUs:', gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ca312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image file paths using tf.data.Dataset\n",
    "# List all the image files with the '.jpg' extension in the 'data/images' directory\n",
    "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the next image from the iterator to a numpy array\n",
    "images.as_numpy_iterator().next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load an image from a file path\n",
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess images using the defined load_image function\n",
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678db175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the next image from the iterator to a numpy array\n",
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b41bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checj image type\n",
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60456cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea89c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image generator from the batched images\n",
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots with 4 columns and a larger figure size\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "\n",
    "# Iterate through the images and their corresponding axes\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image)  # Display the image on the current axis\n",
    "\n",
    "# Show the plot with all the images\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a511a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "images_dir = 'data/images'\n",
    "train_dir = 'data/train/images'\n",
    "test_dir = 'data/test/images'\n",
    "val_dir = 'data/val/images'\n",
    "\n",
    "# Get a list of image file names in the images directory\n",
    "image_files = os.listdir(images_dir)\n",
    "\n",
    "# Shuffle the list of image files randomly\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Calculate the number of images for each partition based on percentages\n",
    "total_images = len(image_files)\n",
    "train_count = int(total_images * 0.7)\n",
    "test_count = int(total_images * 0.15)\n",
    "val_count = total_images - train_count - test_count\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [train_dir, test_dir, val_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Keep track of moved images\n",
    "moved_images = []\n",
    "\n",
    "# Iterate through the shuffled image list and move images to partitions\n",
    "for idx, image_file in enumerate(image_files):\n",
    "    if idx < train_count:\n",
    "        dst_dir = train_dir\n",
    "    elif idx < train_count + test_count:\n",
    "        dst_dir = test_dir\n",
    "    else:\n",
    "        dst_dir = val_dir\n",
    "    \n",
    "    # Move the image file to the appropriate partition directory if not already moved\n",
    "    if image_file not in moved_images:\n",
    "        src_path = os.path.join(images_dir, image_file)\n",
    "        dst_path = os.path.join(dst_dir, image_file)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        moved_images.append(image_file)\n",
    "\n",
    "print(\"Images have been randomly partitioned and moved without replication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iterate through the folders: 'train', 'test', and 'val'\n",
    "for folder in ['train', 'test', 'val']:\n",
    "    # Iterate through the files in the 'images' folder of each folder\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        # Get the filename without extension and create a corresponding JSON filename\n",
    "        filename = file.split('.')[0] + '.json'\n",
    "        existing_filepath = os.path.join('data', 'labels', filename)\n",
    "        \n",
    "        # Check if the JSON file already exists in 'labels' folder\n",
    "        if os.path.exists(existing_filepath):\n",
    "            new_filepath = os.path.join('data', folder, 'labels', filename)\n",
    "            # Replace the existing JSON file with the new one in the respective folder\n",
    "            os.replace(existing_filepath, new_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation pipeline using Albumentations library\n",
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eca652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image using OpenCV\n",
    "img = cv2.imread(os.path.join('data', 'train', 'images', '3a38cf81-4549-11ee-99a6-00e93a59cab9.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df960424",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe24f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON label file\n",
    "with open(os.path.join('data', 'train', 'labels', '3a38cf81-4549-11ee-99a6-00e93a59cab9.json'), 'r') as f:\n",
    "    label = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the list of points from the first shape in the shapes field of the JSON label.\n",
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e914791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from label data\n",
    "coords = [0, 0, 0, 0]  # Initialize the coordinates list\n",
    "coords[0] = label['shapes'][0]['points'][0][0]  # x-coordinate of the first point\n",
    "coords[1] = label['shapes'][0]['points'][0][1]  # y-coordinate of the first point\n",
    "coords[2] = label['shapes'][0]['points'][1][0]  # x-coordinate of the second point\n",
    "coords[3] = label['shapes'][0]['points'][1][1]  # y-coordinate of the second point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f500df",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0efbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the coordinates\n",
    "coords = list(np.divide(coords, [640,480,640,480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27986dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b987d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to the image and bounding box\n",
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check type\n",
    "type(augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check keys\n",
    "augmented.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8557e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "augmented['image'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9492ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a rectangle on the augmented image using the augmented bounding box coordinates\n",
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory names\n",
    "base_dir = 'aug_data'\n",
    "partitions = ['train', 'test', 'val']\n",
    "sub_dirs = ['images', 'labels']\n",
    "\n",
    "# Create the main directory if it doesn't exist\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# Create the subdirectories for each partition\n",
    "for partition in partitions:\n",
    "    partition_dir = os.path.join(base_dir, partition)\n",
    "    if not os.path.exists(partition_dir):\n",
    "        os.makedirs(partition_dir)\n",
    "    for sub_dir in sub_dirs:\n",
    "        sub_dir_path = os.path.join(partition_dir, sub_dir)\n",
    "        if not os.path.exists(sub_dir_path):\n",
    "            os.makedirs(sub_dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b30137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each partition ('train', 'test', 'val')\n",
    "for partition in ['train', 'test', 'val']:\n",
    "    # Loop through each image in the current partition\n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        # Initialize coordinates for the bounding box\n",
    "        coords = [0, 0, 0.00001, 0.00001]\n",
    "        \n",
    "        # Check if a label file exists for the current image\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "            \n",
    "            # Extract bounding box coordinates from label\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            \n",
    "            # Normalize bounding box coordinates\n",
    "            coords = list(np.divide(coords, [640, 480, 640, 480]))\n",
    "\n",
    "        try:\n",
    "            # Augment the image multiple times (60 times in this case)\n",
    "            for x in range(60):\n",
    "                # Apply augmentations to the image and bounding box\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                \n",
    "                # Save the augmented image\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                # Create an annotation dictionary for the augmented image\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                # Check if bounding box exists in augmented image\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0:\n",
    "                        annotation['bbox'] = [0, 0, 0, 0]\n",
    "                        annotation['class'] = 0\n",
    "                    else:\n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else:\n",
    "                    annotation['bbox'] = [0, 0, 0, 0]\n",
    "                    annotation['class'] = 0\n",
    "\n",
    "                # Save the annotation as a JSON file\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image file paths for training\n",
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "\n",
    "# Map the load_image function to decode image files\n",
    "train_images = train_images.map(load_image)\n",
    "\n",
    "# Resize images to a common size\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "train_images = train_images.map(lambda x: x/255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image file paths for training\n",
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "\n",
    "# Map the load_image function to decode image files\n",
    "test_images = test_images.map(load_image)\n",
    "\n",
    "# Resize images to a common size\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "test_images = test_images.map(lambda x: x/255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13253eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image file paths for training\n",
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "\n",
    "# Map the load_image function to decode image files\n",
    "val_images = val_images.map(load_image)\n",
    "\n",
    "# Resize images to a common size\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "val_images = val_images.map(lambda x: x/255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the next image from the iterator to a numpy array\n",
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1deeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    # Load the JSON label file\n",
    "    with open(label_path.numpy(), 'r', encoding=\"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    # Extract the class label and bounding box coordinates\n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb20bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train labels\n",
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "# Map the load_labels function to each label file path\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test labels\n",
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "# Map the load_labels function to each test label file path\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75648996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation labels\n",
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "# Map the load_labels function to each validation label file path\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c725ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the next image from the iterator to a numpy array\n",
    "val_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lengths\n",
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train dataset by zipping together train_images and train_labels\n",
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "\n",
    "# Shuffle the train dataset with a buffer size of 5000\n",
    "train = train.shuffle(5000)\n",
    "\n",
    "# Batch the train dataset with a batch size of 8\n",
    "train = train.batch(8)\n",
    "\n",
    "# Prefetch the train dataset to improve performance by overlapping data preprocessing and model execution\n",
    "train = train.prefetch(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d68ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset by zipping together test_images and test_labels\n",
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "\n",
    "# Shuffle the test dataset with a buffer size of 1300\n",
    "test = test.shuffle(1300)\n",
    "\n",
    "# Batch the test dataset with a batch size of 8\n",
    "test = test.batch(8)\n",
    "\n",
    "# Prefetch the test dataset to improve performance by overlapping data preprocessing and model execution\n",
    "test = test.prefetch(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f492c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation dataset by zipping together val_images and val_labels\n",
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "\n",
    "# Shuffle the validation dataset with a buffer size of 1000\n",
    "val = val.shuffle(1000)\n",
    "\n",
    "# Batch the validation dataset with a batch size of 8\n",
    "val = val.batch(8)\n",
    "\n",
    "# Prefetch the validation dataset to improve performance by overlapping data preprocessing and model execution\n",
    "val = val.prefetch(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dad9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the shape of the first image in the next batch from the training dataset\n",
    "train.as_numpy_iterator().next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1df087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the labels from the next batch in the training dataset\n",
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for the training dataset\n",
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ef65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the next batch of data from the iterator\n",
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a figure with 4 subplots\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "\n",
    "# Iterate over the first 4 samples in the batch\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "#     Draw a rectangle around the detected face\n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "#     Display the sample image in the corresponding subplot\n",
    "    ax[idx].imshow(sample_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model without the fully connected layers (top)\n",
    "vgg = VGG16(include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fa8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary\n",
    "vgg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    # Define the input layer with shape (120, 120, 3)\n",
    "    input_layer = Input(shape=(120, 120, 3))\n",
    "    \n",
    "    # Load the VGG16 model without the fully connected layers (top)\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model: GlobalMaxPooling2D followed by Dense layers\n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model: GlobalMaxPooling2D followed by Dense layers\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    # Create the combined model with both classification and bounding box outputs\n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the facetracker model using the build_model function\n",
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the facetracker model's architecture\n",
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch of training data\n",
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e76923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the facetracker model\n",
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of batches per epoch\n",
    "batches_per_epoch = len(train)\n",
    "\n",
    "# Calculate the learning rate decay based on the specified formula\n",
    "lr_decay = (1./0.75 - 1) / batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3de6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an Adam optimizer with the specified learning rate and decay\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Adam optimizer with the specified learning rate and decay\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001, decay=lr_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf249a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    # Calculate the squared difference between true and predicted coordinates\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:, :2] - yhat[:, :2]))\n",
    "\n",
    "    # Calculate the height and width of true and predicted bounding boxes\n",
    "    h_true = y_true[:, 3] - y_true[:, 1]\n",
    "    w_true = y_true[:, 2] - y_true[:, 0]\n",
    "\n",
    "    h_pred = yhat[:, 3] - yhat[:, 1]\n",
    "    w_pred = yhat[:, 2] - yhat[:, 0]\n",
    "\n",
    "    # Calculate the squared difference in size of bounding boxes\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "\n",
    "    # Return the sum of squared coordinate difference and size difference\n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification loss using Binary Crossentropy\n",
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Define the localization loss using your previously defined localization_loss function\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92966d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3738f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FaceTracker model as a subclass of tf.keras.Model\n",
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    # Override the compile method to set loss functions and optimizer\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    # Override the training step to define the training process\n",
    "    def train_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        # Use GradientTape to compute gradients\n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        # Apply gradients using the optimizer\n",
    "        self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        # Return loss values for monitoring\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "    \n",
    "    # Override the testing step to define the testing process\n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "        \n",
    "        # Return loss values for monitoring\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "        \n",
    "    # Override the call method to pass inputs through the model\n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the FaceTracker model\n",
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimizer and loss functions\n",
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the log directory for TensorBoard\n",
    "logdir = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de985a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback to log training progress\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10edbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the train dataset and include the TensorBoard callback\n",
    "model.fit(train, epochs=num_epochs, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b62777",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "# Plotting the total loss and validation loss\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')  # Set title for the subplot\n",
    "ax[0].legend()  # Add legend to the plot\n",
    "\n",
    "# Plotting the classification loss and validation classification loss\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')  # Set title for the subplot\n",
    "ax[1].legend()  # Add legend to the plot\n",
    "\n",
    "# Plotting the regression loss and validation regression loss\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')  # Set title for the subplot\n",
    "ax[2].legend()  # Add legend to the plot\n",
    "\n",
    "plt.show()  # Display the subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09051fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an iterator for the test dataset\n",
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the face detection model to predict on the test batch\n",
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test samples with predicted face bounding boxes\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained facetracker model\n",
    "facetracker.save('facetracker.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved facetracker model\n",
    "facetracker = load_model('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the face detection model\n",
    "facetracker = tf.keras.models.load_model('facetracker.h5')\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Crop the frame to a specific region\n",
    "    frame = frame[50:500, 50:500, :]\n",
    "\n",
    "    # Convert the frame to RGB and resize it for processing\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120, 120))\n",
    "\n",
    "    # Predict using the face detection model\n",
    "    yhat = facetracker.predict(np.expand_dims(resized / 255, 0))\n",
    "    sample_coords = yhat[1][0]\n",
    "\n",
    "    if yhat[0] > 0.5:\n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame,\n",
    "                      tuple(np.multiply(sample_coords[:2], [450, 450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450, 450]).astype(int)),\n",
    "                      (255, 0, 0), 2)\n",
    "        \n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame,\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                   [0, -30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                   [80, 0])),\n",
    "                      (255, 0, 0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                                 [0, -5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the modified frame\n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
